{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5272e94c",
   "metadata": {},
   "source": [
    "# RAG Research Summarizer with Claude (Proof of Concept)\n",
    "This script uses Anthropic's Claude to answer queries using relevant research summaries.\n",
    "\n",
    "## Setup:\n",
    "1. Add your API key to a file called ignore.py at the same directory level as this script:\n",
    "\n",
    "    KEY = \"your_claude_api_key_here\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf881731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq torch sentence-transformers anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e298a7c2",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8000a0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jrosenb8/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/jrosenb8/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import ignore\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import anthropic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22617b3b",
   "metadata": {},
   "source": [
    "# extract documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5551ddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_001||A longitudinal study of 1,200 middle school students found that incorporating spaced repetition into mathematics curriculum improved long-term retention by 42% compared to massed practice. Students who reviewed concepts at intervals of 1, 3, and 7 days showed significantly better performance on assessments administered three months later.\n",
      "\n",
      "doc_002||Research examining 500 undergraduate students revealed that handwritten notes led to 23% better conceptual understanding compared to laptop note-taking. The constraint of slower handwriting appeared to force students to process and synthesize information more deeply during lectures.\n",
      "\n",
      "doc_003||A meta-analysis of 74 studies found that peer tutoring programs increased academic achievement by an average effect size of 0.59 standard deviations. Benefits were particularly pronounced when tutors were trained in questioning techniques and given structured materials.\n",
      "\n",
      "doc_004||Investigation of 15 elementary schools implementing project-based learning showed mixed results: while student engagement increased by 67%, standardized test scores showed no significant improvement in the first two years of implementation. Teachers reported needing 18-24 months to effectively adapt their practice.\n",
      "\n",
      "doc_005||Analysis of online learning outcomes during 2020-2021 revealed that synchronous video classes with breakout rooms achieved 89% of the learning gains of in-person instruction, while asynchronous-only formats achieved only 64%. Student isolation was the primary predictor of poor outcomes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = json.load(open('./documents.json', 'r'))\n",
    "docs = [f'{k}||{v}' for k, v in docs.items()]  # Make them a list with some metadata fusion.\n",
    "\n",
    "for doc in docs[:5]:  # print a few docs as an example\n",
    "    print(doc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34302a0e",
   "metadata": {},
   "source": [
    "# cosine similarity function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92af8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_top_k(model: SentenceTransformer, query: str, doc_embs: torch.Tensor, docs: list[str], k: int = 3) -> list[tuple[float, str]]:\n",
    "    \"\"\"\n",
    "    Perform a cosine similarity search for a query against precomputed document embeddings.\n",
    "\n",
    "    Args:\n",
    "        model (SentenceTransformer): Preloaded Huggingface embedding model.\n",
    "        query (str): Query string.\n",
    "        doc_embs (torch.Tensor): Precomputed document embeddings (normalized).\n",
    "        docs (List[str]): Original documents corresponding to embeddings.\n",
    "        k (int, optional): Number of top results to return. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        list[Tuple[float, str]]: List of (similarity_score, document) tuples.\n",
    "    \"\"\"\n",
    "    query_emb = model.encode([query], convert_to_tensor=True, normalize_embeddings=True)\n",
    "    sims = util.cos_sim(query_emb, doc_embs)[0]  # shape: [num_docs]\n",
    "    top_k = torch.topk(sims, k=k)\n",
    "    return [(score.item(), docs[idx]) for idx, score in zip(top_k.indices, top_k.values)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b34cce6",
   "metadata": {},
   "source": [
    "# build model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2855932",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "doc_embs = model.encode(docs, convert_to_tensor=True, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26b30a",
   "metadata": {},
   "source": [
    "### example search usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9dd3a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5860 | doc_012||Research on outdoor education programs found that students who spent at least 2 hours per week in nature-based learning showed 26% reduction in stress markers and 18% improvement in creative problem-solving tasks compared to indoor-only control groups.\n",
      "\n",
      "0.3282 | doc_004||Investigation of 15 elementary schools implementing project-based learning showed mixed results: while student engagement increased by 67%, standardized test scores showed no significant improvement in the first two years of implementation. Teachers reported needing 18-24 months to effectively adapt their practice.\n",
      "\n",
      "0.2837 | doc_005||Analysis of online learning outcomes during 2020-2021 revealed that synchronous video classes with breakout rooms achieved 89% of the learning gains of in-person instruction, while asynchronous-only formats achieved only 64%. Student isolation was the primary predictor of poor outcomes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'what do we know about outdoor education'\n",
    "n = 3\n",
    "\n",
    "results = search_top_k(model, query, doc_embs, docs, k=n)\n",
    "\n",
    "for score, doc in results:\n",
    "    print(f\"{score:.4f} | {doc}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a167ffd",
   "metadata": {},
   "source": [
    "# prompt building and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27dd541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(query: str, sources: int = 3, print_flag: bool = False) -> str:\n",
    "    results = search_top_k(model, query, doc_embs, docs, k=sources)\n",
    "\n",
    "    if print_flag:\n",
    "        for score, doc in results:\n",
    "            print(f\"{score:.4f} | {doc}\")\n",
    "\n",
    "\n",
    "    rag_input = {\n",
    "        \"query\": query,\n",
    "        \"research_summaries\": [\n",
    "            {\n",
    "                \"score\": score,\n",
    "                \"id\": text.split('||')[0],\n",
    "                \"text\": text.split('||')[1]\n",
    "            }\n",
    "            for score, text in results[:n]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assisntant that uses retrieval augmented generation to answer questions about educational best practices\n",
    "\n",
    "    == Relevant Information ==\n",
    "    Reference Summaries: You will be provided with structured summaries of research papers.\n",
    "    Relevance Filtering: Only use information from the summaries if it is directly relevant to the query.\n",
    "    Answer Generation: Generate concise, accurate, and clear answers to the user query.\n",
    "    Citation: When using information from a summary, include a reference to the summary’s ID.\n",
    "\n",
    "    ==INPUT==\n",
    "    {json.dumps(rag_input, indent=2)}\n",
    "\n",
    "    ==EXAMPLE OUTPUT== \n",
    "    {{\n",
    "    \"answer\": <\"Answer based on relevant summaries.\">,\n",
    "    \"used_summaries\": <[\"id1\", ..., \"idn\"]>\n",
    "    }}\n",
    "\n",
    "    ==IMPORTANT==\n",
    "    - Only respond with the output JSON, nothing before or after; DO NOT inlude \"```json\" or other markdown in your response.\n",
    "    - Maintain a professional and friendly tone.\n",
    "    - Respond only by referencing the given input. If none of the input is relevant to the user query, then respond that you have nothing useful to say.\n",
    "    - Do not elaborate at all in your response outside of the input data.\n",
    "    - Be concise\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59cbeb",
   "metadata": {},
   "source": [
    "### putting it all together with claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dd6cd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Research on class size reduction from 25 to 15 students showed a 12% improvement in elementary reading scores, though no significant effect was observed in mathematics achievement. However, cost-benefit analysis indicated that targeted tutoring might be a more efficient approach than reducing class sizes (doc_016).', 'used_summaries': ['doc_016']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = generate_prompt(query='Tell me about optimal class size?')\n",
    "\n",
    "client = anthropic.Anthropic(api_key=ignore.KEY)\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",    \n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "response_obj = json.loads(response.content[0].text)\n",
    "\n",
    "print(response_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c2cb9",
   "metadata": {},
   "source": [
    "# A more production style oop example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "708591ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPromptGenerator:\n",
    "    def __init__(self, docs: list[str], api_key: str, embedding_model: str = \"all-MiniLM-L6-v2\", claude_model: str = \"claude-sonnet-4-5-20250929\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG prompt generator and embed the documents.\n",
    "\n",
    "        Args:\n",
    "            docs: List of documents with format \"id||text\".\n",
    "            embedding_model: Name of the SentenceTransformer model to use for embeddings.\n",
    "            claude_model: Which Claude model to use.\n",
    "        \"\"\"\n",
    "        self.docs = docs\n",
    "        self.model = SentenceTransformer(embedding_model)\n",
    "        self.doc_embs = self.model.encode(docs, convert_to_tensor=True, normalize_embeddings=True)\n",
    "        self.claude_model = claude_model\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "    def search_top_k(self, query: str, k: int = 3) -> list[tuple[float, str]]:\n",
    "        \"\"\"Perform a cosine similarity search for a query against precomputed document embeddings.\"\"\"\n",
    "        query_emb = self.model.encode([query], convert_to_tensor=True, normalize_embeddings=True)\n",
    "        sims = util.cos_sim(query_emb, self.doc_embs)[0]\n",
    "        top_k = torch.topk(sims, k=k)\n",
    "        return [(score.item(), self.docs[idx]) for idx, score in zip(top_k.indices, top_k.values)]\n",
    "\n",
    "    def generate_prompt(self, query: str, sources: int = 3, print_flag: bool = False) -> str:\n",
    "        \"\"\"Generate a RAG-style prompt with top-k relevant research summaries.\"\"\"\n",
    "        results = self.search_top_k(query, k=sources)\n",
    "\n",
    "        if print_flag:\n",
    "            for score, doc in results:\n",
    "                print(f\"{score:.4f} | {doc}\")\n",
    "\n",
    "        rag_input = {\n",
    "            \"query\": query,\n",
    "            \"research_summaries\": [\n",
    "                {\n",
    "                    \"score\": score,\n",
    "                    \"id\": text.split('||')[0],\n",
    "                    \"text\": text.split('||')[1]\n",
    "                }\n",
    "                for score, text in results[:sources]\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant that uses retrieval-augmented generation to answer questions about educational best practices.\n",
    "\n",
    "        == Relevant Information ==\n",
    "        Reference Summaries: You will be provided with structured summaries of research papers.\n",
    "        Relevance Filtering: Only use information from the summaries if it is directly relevant to the query. You may use mutliple summaries if they are all relevant.\n",
    "        Answer Generation: Generate concise, accurate, and clear answers to the user query.\n",
    "        Citation: When using information from a summary, include a reference to the summary’s ID.\n",
    "\n",
    "        ==INPUT==\n",
    "        {json.dumps(rag_input, indent=2)}\n",
    "\n",
    "        ==EXAMPLE OUTPUT==\n",
    "        {{\n",
    "        \"answer\": <\"Answer based on relevant summaries.\">,\n",
    "        \"used_summaries\": <[\"id1\", ..., \"idn\"]>,\n",
    "        \"all_summaries\": <[\"id1\", ..., \"idn\"]>\n",
    "        }}\n",
    "\n",
    "        ==IMPORTANT==\n",
    "        - Only respond with the output JSON, nothing before or after; DO NOT inlude \"```json\" or other markdown in your response.\n",
    "        - Maintain a professional and friendly tone.\n",
    "        - Respond only by referencing the given input. If none of the input is relevant to the user query, then respond that you have nothing useful to say.\n",
    "        - Do not elaborate at all in your response outside of the input data.\n",
    "        - Be concise\n",
    "\n",
    "        *REMEBER* \n",
    "        - Your response **must be valid JSON only**.\n",
    "        - DO NOT include ```json, ``` or any other markdown syntax.\n",
    "        - Do NOT include explanations, greetings, or extra text—only the JSON.\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def query_llm(self, query: str, sources: int = 3, print_flag: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Full pipeline: query -> retrieve top summaries -> generate prompt -> call Claude -> return JSON.\n",
    "        \"\"\"\n",
    "        prompt = self.generate_prompt(query, sources=sources, print_flag=print_flag)\n",
    "\n",
    "        response = self.client.messages.create(\n",
    "            model=self.claude_model,\n",
    "            max_tokens=1024,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = response.content[0].text.strip(r'```json').strip(r'```')\n",
    "            response_obj = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            response_obj = {\"error\": \"Failed to parse response JSON\", \"raw_text\": response}\n",
    "\n",
    "        return response_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6276bda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"Research on outdoor classes shows significant benefits for students. Students who participated in nature-based learning for at least 2 hours per week demonstrated a 26% reduction in stress markers and an 18% improvement in creative problem-solving tasks compared to students in indoor-only settings (doc_012).\",\n",
      "  \"used_summaries\": [\n",
      "    \"doc_012\"\n",
      "  ],\n",
      "  \"all_summaries\": [\n",
      "    \"doc_012\",\n",
      "    \"doc_005\",\n",
      "    \"doc_014\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "rag_generator = RAGPromptGenerator(docs, \n",
    "                                   api_key=ignore.KEY,\n",
    "                                   embedding_model='all-MiniLM-L6-v2', \n",
    "                                   claude_model='claude-sonnet-4-5-20250929')\n",
    "\n",
    "query = \"What do we know about classes that are outdoors?\"\n",
    "response = rag_generator.query_llm(query, sources=3)\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6013a799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"Research shows that allowing code-switching between students' native language and English during initial concept introduction can significantly improve comprehension. A study of 1,500 ESL students found that this approach improved comprehension by 44%, which challenges the effectiveness of English-only immersion policies (doc_013).\",\n",
      "  \"used_summaries\": [\n",
      "    \"doc_013\"\n",
      "  ],\n",
      "  \"all_summaries\": [\n",
      "    \"doc_013\",\n",
      "    \"doc_010\",\n",
      "    \"doc_016\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I help my students who speak english as a second language?\"\n",
    "response = rag_generator.query_llm(query, sources=3)\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7287d07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"Combining meditation with outdoor settings could be beneficial for your students. Research shows that mindfulness meditation in high school students led to a 15% reduction in test anxiety and 8% improvement in test scores, though programs need to be at least 8 weeks long to be effective (doc_020). Additionally, students who spent at least 2 hours per week in nature-based learning showed a 26% reduction in stress markers and 18% improvement in creative problem-solving tasks (doc_012). While these studies examined meditation and outdoor education separately, both approaches show positive effects on student wellbeing and performance.\",\n",
      "  \"used_summaries\": [\n",
      "    \"doc_020\",\n",
      "    \"doc_012\"\n",
      "  ],\n",
      "  \"all_summaries\": [\n",
      "    \"doc_020\",\n",
      "    \"doc_012\",\n",
      "    \"doc_019\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = \"Would meditating outside be useful to my students?\"\n",
    "response = rag_generator.query_llm(query, sources=3)\n",
    "print(json.dumps(response, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
